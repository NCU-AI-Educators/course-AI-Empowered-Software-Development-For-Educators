import os
# Fix for MPS 'aten::isin' error: Enable fallback to CPU for unsupported ops
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
# Suppress huggingface/tokenizers parallelism warning
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import uvicorn
from fastapi import FastAPI, UploadFile, File, Body
from fastapi.responses import HTMLResponse, Response
from pydantic import BaseModel
from transformers import AutoProcessor, AutoModelForCausalLM
from PIL import Image
import io
import socket
import qrcode
import sys
import torch
import pathlib
import pathlib
import time
import warnings # Added for warning suppression
from openai import OpenAI  # <--- New Import for Lesson 26
from fastapi.responses import StreamingResponse # For Audio Stream

# Filter out specific library warnings to keep console clean for students
warnings.filterwarnings("ignore", message=".*To copy construct from a tensor.*")
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")

# 1. åˆå§‹åŒ– FastAPI åº”ç”¨
app = FastAPI(title="Lesson 26 AI Vision Translator")

# ==========================================
# â˜ï¸ Cloud API Configuration (Lesson 26 New)
# ==========================================
# å°è¯•ä»ç¯å¢ƒå˜é‡è·å– Keyï¼Œå¦‚æœæ²¡æœ‰åˆ™ç•™ç©º (ä¼šæŠ¥é”™æç¤º)
# å°è¯•ä»ç¯å¢ƒå˜é‡è·å– Key
SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ Key
VALID_API_KEY = SILICONFLOW_API_KEY or OPENAI_API_KEY

SILICONFLOW_BASE_URL = "https://api.siliconflow.cn/v1"
CLOUD_MODEL_NAME = "deepseek-ai/DeepSeek-V3.2" 

if not VALID_API_KEY:
    print("\n" + "!"*50)
    print("â›”ï¸ è‡´å‘½é”™è¯¯: æœªæ£€æµ‹åˆ° API Keyï¼")
    print("--------------------------------------------------")
    print("è¯·è‡³å°‘è®¾ç½®ä»¥ä¸‹å…¶ä¸­ä¸€ä¸ªç¯å¢ƒå˜é‡:")
    print("1. export SILICONFLOW_API_KEY='sk-...' (æ¨è)")
    print("2. export OPENAI_API_KEY='sk-...' (å¤‡é€‰)")
    print("!"*50 + "\n")
    sys.exit(1) # Fail fast: ç¼ºå°‘æ ¸å¿ƒä¾èµ–ç›´æ¥é€€å‡º

# Mock flash_attn for Mac compatibility
from unittest.mock import MagicMock
sys.modules["flash_attn"] = MagicMock()
sys.modules["flash_attn"].__spec__ = MagicMock()

# Helper: Get Local IP
def get_local_ip():
    try:
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.connect(("8.8.8.8", 80))
        ip = s.getsockname()[0]
        s.close()
        return ip
    except:
        return "127.0.0.1"

# 2. é¢„åŠ è½½æ¨¡å‹ (ä½¿ç”¨ Microsoft Florence-2-base)
local_model_path = "./models/florence-2-base"
if os.path.exists(local_model_path):
    print(f"ğŸ“‚ å‘ç°æœ¬åœ°æ¨¡å‹ path: {local_model_path}")
    model_id = local_model_path
else:
    print(f"âš ï¸ æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹ï¼Œå‡†å¤‡ä» HuggingFace ä¸‹è½½...")
    model_id = "microsoft/Florence-2-base"

device = "mps" if torch.backends.mps.is_available() else "cpu"
torch_dtype = torch.float16 if device != "cpu" else torch.float32

model = None
processor = None

try:
    print("="*50)
    print(f"ğŸš€ æ­£åœ¨åŠ è½½ Florence-2 æ¨¡å‹...")
    print(f"ğŸ“‚ æ¨¡å‹æ¥æº: {model_id}")
    print(f"ğŸ–¥ï¸  è¿è¡Œè®¾å¤‡: {device.upper()}")
    
    local_files_only = False
    if os.path.isdir(model_id):
        print(f"ğŸ”Œ æ£€æµ‹åˆ°æœ¬åœ°è·¯å¾„ï¼Œå¯ç”¨ç¦»çº¿æ¨¡å¼ (local_files_only=True)")
        local_files_only = True
        os.environ["HF_DATASETS_OFFLINE"] = "1"
        os.environ["TRANSFORMERS_OFFLINE"] = "1"
    
    model = AutoModelForCausalLM.from_pretrained(
        model_id, 
        trust_remote_code=True, 
        torch_dtype=torch_dtype,
        local_files_only=local_files_only
    ).to(device)
    
    processor = AutoProcessor.from_pretrained(
        model_id, 
        trust_remote_code=True,
        local_files_only=local_files_only
    )
    
    print("âœ¨ Florence-2 æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    print("="*50)
except Exception as e:
    print(f"\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

# 3. æ¨¡å‹é¢„çƒ­
def warmup_model():
    if not model or not processor: return
    print("ğŸ”¥ æ­£åœ¨é¢„çƒ­æ¨¡å‹... (æ¶ˆé™¤é¦–æ¬¡æ¨ç†å¡é¡¿)")
    try:
        dummy_img = Image.new('RGB', (64, 64), color='white')
        dummy_prompt = "<CAPTION>"
        inputs = processor(text=dummy_prompt, images=dummy_img, return_tensors="pt").to(device, torch_dtype)
        model.generate(
            input_ids=inputs["input_ids"],
            pixel_values=inputs["pixel_values"],
            max_new_tokens=5, 
            do_sample=False,
            num_beams=1,
        )
        print("âœ… æ¨¡å‹é¢„çƒ­å®Œæˆï¼")
    except Exception as e:
        print(f"âš ï¸ é¢„çƒ­å¤±è´¥ (ä¸å½±å“ä¸»åŠŸèƒ½): {e}")

warmup_model()

# ==========================================
# ğŸ§  new helper: Cloud Translation
# ==========================================
def translate_text(text: str) -> str:
    """è°ƒç”¨äº‘ç«¯å¤§æ¨¡å‹å°†è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡"""
    # å°è¯•å¤šç§æ–¹å¼è·å– Key
    api_key = SILICONFLOW_API_KEY or os.getenv("OPENAI_API_KEY")
    
    # å³ä½¿ api_key ä¸º Noneï¼Œæˆ‘ä»¬ä¹Ÿå°è¯•åˆå§‹åŒ–ï¼Œå› ä¸º OpenAI SDK å¯èƒ½æœ‰è‡ªå·±çš„é…ç½®åŠ è½½æœºåˆ¶
    try:
        client = OpenAI(api_key=api_key, base_url=SILICONFLOW_BASE_URL)
        response = client.chat.completions.create(
            model=CLOUD_MODEL_NAME,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚è¯·å°†ç”¨æˆ·çš„è‹±æ–‡è¾“å…¥ç›´æ¥ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€‚"},
                {"role": "user", "content": text},
            ],
            stream=False
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ ç¿»è¯‘å‡ºé”™: {e}")
        return f"ç¿»è¯‘å¤±è´¥: {str(e)} (å¯èƒ½æ˜¯ API Key æœªé…ç½®)"

        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ ç¿»è¯‘å‡ºé”™: {e}")
        return f"ç¿»è¯‘å¤±è´¥: {str(e)} (å¯èƒ½æ˜¯ API Key æœªé…ç½®)"

# Pydantic Model for API
class TranslationRequest(BaseModel):
    text: str

class TTSRequest(BaseModel):
    text: str

# ==========================================
# ğŸš¦ Routes
# ==========================================

@app.get("/", response_class=HTMLResponse)
def home():
    """Serve the frontend HTML"""
    return pathlib.Path("lesson26.html").read_text(encoding="utf-8")

@app.get("/qrcode")
def get_qr_image():
    ip = get_local_ip()
    url = f"http://{ip}:8000"
    qr = qrcode.QRCode(box_size=10, border=4)
    qr.add_data(url)
    qr.make(fit=True)
    img = qr.make_image(fill_color="black", back_color="white")
    buf = io.BytesIO()
    img.save(buf, format="PNG")
    buf.seek(0)
    return Response(content=buf.getvalue(), media_type="image/png")

@app.post("/upload")
async def upload_image(file: UploadFile = File(...)):
    """
    Step 1: è§†è§‰è¯†åˆ« (Vision Only)
    æ¥æ”¶å›¾ç‰‡ -> Florence-2 ç”Ÿæˆè‹±æ–‡æè¿° -> è¿”å›è‹±æ–‡
    """
    global model, processor
    
    if not model or not processor:
        return {"error": "Model not loaded."}

    try:
        contents = await file.read()
        image = Image.open(io.BytesIO(contents))
        if image.mode != "RGB":
            image = image.convert("RGB")
        
        # æ„é€ æç¤ºè¯ task
        prompt = "<MORE_DETAILED_CAPTION>"
        
        # æ‰“å°æ—¥å¿—
        print(f"\nğŸ“¸ æ”¶åˆ°å›¾ç‰‡: {file.filename}")
        
        # è®¡æ—¶å¼€å§‹
        start_time = time.time()
        
        # 1. æœ¬åœ°è§†è§‰æ¨ç† (Florence-2)
        inputs = processor(text=prompt, images=image, return_tensors="pt").to(device, torch_dtype)
        generated_ids = model.generate(
            input_ids=inputs["input_ids"],
            pixel_values=inputs["pixel_values"],
            max_new_tokens=1024,
            do_sample=False,
            num_beams=3,
        )
        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
        parsed_answer = processor.post_process_generation(
            generated_text, 
            task=prompt, 
            image_size=(image.width, image.height)
        )
        caption_en = parsed_answer[prompt]
        
        # è®¡æ—¶ç»“æŸ
        end_time = time.time()
        cost_time = round(end_time - start_time, 2)
        
        # æ‰“å°æ—¥å¿—
        print(f"â±ï¸ è§†è§‰è€—æ—¶: {cost_time}s")
        print(f"ğŸ¤– EN: {caption_en}")
        
        return {
            "label": caption_en,     # å…¼å®¹æ—§é€»è¾‘ï¼Œå‰ç«¯å¦‚æœæ˜¯æ—§ç‰ˆä¹Ÿä¼šç›´æ¥æ˜¾ç¤ºè‹±æ–‡
            "caption_en": caption_en, # æ˜¾å¼å­—æ®µ
            "cost_time": cost_time
        }
    except Exception as e:
        return {"error": str(e)}

@app.post("/translate")
async def translate_endpoint(request: TranslationRequest):
    """
    Step 2: äº‘ç«¯ç¿»è¯‘ (Language Only)
    æ¥æ”¶è‹±æ–‡ -> è°ƒç”¨ API -> è¿”å›ä¸­æ–‡
    """
    print(f"â˜ï¸ æ”¶åˆ°ç¿»è¯‘è¯·æ±‚: {request.text[:50]}...")
    start_time = time.time()
    
    caption_zh = translate_text(request.text)
    
    end_time = time.time()
    cost_time = round(end_time - start_time, 2)
    
    print(f"ğŸ‡¨ğŸ‡³ ZH: {caption_zh}")
    print(f"â±ï¸ ç¿»è¯‘è€—æ—¶: {cost_time}s")
    
    return {
        "caption_zh": caption_zh,
        "cost_time": cost_time
    }

@app.post("/speak")
async def speak_endpoint(request: TTSRequest):
    """
    Step 3: æ–‡æœ¬è½¬è¯­éŸ³ (TTS)
    æ¥æ”¶ä¸­æ–‡ -> è°ƒç”¨ SiliconFlow API -> è¿”å› MP3 éŸ³é¢‘æµ
    """
    print(f"ğŸ”ˆ æ”¶åˆ° TTS è¯·æ±‚: {request.text[:50]}...")
    
    # å°è¯•å¤šç§æ–¹å¼è·å– Key
    api_key = SILICONFLOW_API_KEY or OPENAI_API_KEY
    client = OpenAI(api_key=api_key, base_url=SILICONFLOW_BASE_URL)

    def generate_audio():
        with client.audio.speech.with_streaming_response.create(
            model="FunAudioLLM/CosyVoice2-0.5B", 
            voice="FunAudioLLM/CosyVoice2-0.5B:anna", # éŸ³è‰²
            input=request.text, 
            response_format="mp3"
        ) as response:
            for chunk in response.iter_bytes():
                yield chunk

    return StreamingResponse(generate_audio(), media_type="audio/mpeg")

if __name__ == "__main__":
    ip = get_local_ip()
    port = 8000
    url = f"http://{ip}:{port}/docs"
    
    print("\n" + "="*50)
    print(f"ğŸš€ Lesson 26 AI Vision Translator å¯åŠ¨ä¸­...")
    print(f"Running on: {url}")
    print("="*50)
    
    # æ‰“å°äºŒç»´ç ä¾›æ‰«æ
    try:
        qr = qrcode.QRCode()
        qr.add_data(url)
        qr.print_ascii(invert=True)
        print(f"\nğŸ“± æ‰‹æœºæ‰«ç ä½“éªŒ: {url}")
    except:
        pass
    
    print("\næŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    
    uvicorn.run(app, host="0.0.0.0", port=port)
